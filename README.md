This project is (loosely) built around the metaphor that we are at a bazaar and we are haggling for computation. We can trade memory for time. Different stalls offer different trade-offs.

## (Distributed) Specialists

The key to efficiency is specializing. (Why?)

Want a system that decomposes a problem and routes the subproblems to a series of specialists. A GPU, CPU, TPU, QPU, ?PU, ...

How can computers be made more efficient (in time, energy, memory, ...) by using different basic units (fused multiply add, ...)? TPUs vs GPUs vs ?!?
How can we make cloud services such as AWS or Google or Azure, ... more efficient by using the large scale to make (super) specialists.
How does the brain solve this problem? What specialists does it have? How does it decompose problems?
In the typical setting this requires knowledge about what sort of tasks are going to be required to specialise in. (reminds me of entropy - want to allocate speedups/efficiency gains based on likelihood of that task) (aka Must assume that current tasks and not going to change.)

Can we extended this to a system that learns from experience which tasks are required and adapts its structure to make itself more efficient.

### Self-assembling computers

Claim: the best properties of brains (aka biological computers) are that they;
* assemble themselves
* are constructed from cheap materials
* have low power requirements
* are robust to damage
* are adaptive.
Can we develop computers capable of learning, that assemble themselves from cheap, everyday materials and require little energy to function? This would require us to be able to write down a very small set of procedural rules that can construct a general purpose learning system!

Benefits

Everyone would be able to 'grow' and run their own computers. Open source computers!
Having reduced learning to a small set of efficient rules would mean that we truly understand the necessary requirements of learning.
